# Hand Gesture Recognition 
# Using HoloLens 2 and NVIDIA Jetson

## Project Overview
This repository involves research artifacts for the paper _“Did I Do Well? Personalized Assessment of Trainees' Performance in Augmented Reality-assisted Neurosurgical Training”_, submitted to **the 5th Annual Workshop on 3D Content Creation for Simulated Training in eXtended Reality, co-located with IEEE VR, 2024** by [Sarah Eom](https://sites.duke.edu/sangjuneom/), [Tiffany Ma](https://sites.duke.edu/tiffanyma/), [Tianyi Hu](http://hutianyi.tech/), Neha Vutakuri, Joshua Jackson, and [Maria Gorlatova](https://maria.gorlatova.com/). The repository contains implementations of collecting hand landmarks using HoloLens 2 and processing them through a deep learning model on an NVIDIA Jetson board for hand gesture recognition. This README provides information on how to set up and use the project.

## Table of Contents
- [Installation](#installation)
- [Hardware Requirements](#hardware-requirements)
- [Software Requirements](#software-requirements)
- [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## Installation

Instructions for installing the project. Include steps for cloning the repo, setting up environments, etc.

```bash
git clone https://github.com/yourusername/hand-gesture-recognition.git
# Add additional setup steps
```

## Hardware Requirements

- HoloLens 2
- NVIDIA Jetson (specify models supported)

## Software Requirements

    List any software requirements, including specific libraries or frameworks needed.

## Setup

Detailed instructions on setting up the HoloLens 2 for hand landmark collection and configuring the NVIDIA Jetson board for running the deep learning model.
HoloLens 2 Setup

    Step 1
    Step 2
    ...

## NVIDIA Jetson Setup

    Step 1
    Step 2
    ...

## Usage

How to use the project. Include instructions on starting the application, any command line arguments, etc.

bash


## Example command to run the project
python3 hand_gesture_recognition.py

## Contributing

Guidelines for contributing to the project.


# Associated Demo
The associated demo, _"Did You Do Well? Real-Time Personalized Feedback on Catheter Placement in Augmented Reality-assisted Neurosurgical Training"_, presents an AR-assisted neurosurgical training tool that provides real-time personalized feedback based on trainees' manipulation of the surgical environment and eye gaze patterns. The video of this demo can be found on [YouTube](https://youtu.be/AKNKKrCvapI). 

# Citation
Please cite the following paper in your publications if this code helps your research.
```
@inproceedings{Eom24ARNeuro,
  title={Did I Do Well? Personalized Assessment of Trainees' Performance in Augmented Reality-assisted Neurosurgical Training},
  author={Eom, Sangjun and Ma, Tiffany and Hu, Tianyi and Vutakuri, Neha and Jackson, Joshua and Gorlatova, Maria},
  booktitle={Proc. IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  year={2024},
  organization={IEEE}
}
```
# Acknowledgments
The contributors of the code are [Tianyi Hu](http://hutianyi.tech/) and [Maria Gorlatova](https://maria.gorlatova.com/). For questions on this repository or the related paper, please contact Tianyi Hu at tianyi.hu [AT] duke [DOT] edu.

This work was supported in part by NSF grants CNS-2112562 and CNS-1908051, NSF CAREER Award IIS-2046072, and by a Thomas Lord Educational Innovation Grant.

# License

State the license under which your project is available. Common choices include MIT, GPL, or Apache 2.0.
Acknowledgments

    Mention any individuals or organizations that contributed to the project.
    Any third-party assets or code used in the project.
    Any other acknowledgments.
