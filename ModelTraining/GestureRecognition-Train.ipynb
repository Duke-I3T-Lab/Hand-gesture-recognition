{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from utlis import tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "dataPath = './dataset/data_combined.csv'\n",
    "\n",
    "# size of sliding window\n",
    "lagWindowSize=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_preprocess_data(dataPath, lagWindowSize=10):\n",
    "    ## 1. Load labelled data\n",
    "    print(\"Load Labelled Data\")\n",
    "    #X, y = tools.load_labelled_data(dataPath, isLeft=True)\n",
    "    X, y = tools.load_labelled_data_combine(dataPath)\n",
    "    \n",
    "    ## 2. Generate Time-lagged data\n",
    "    print(\"Generate Time-lagged Data\")\n",
    "    X_lag, y_lag = tools.generate_time_lags(X, y, lagWindowSize)\n",
    "    ## 3. Convert 2D dataframe to 3D numpy array: (Batch, TimeLag, Features)\n",
    "    print(\"Convert 2D dataframe to 3D numpy array\")\n",
    "    X_3D = tools.convert_df_2_np_3D(X_lag, lagWindowSize)\n",
    "    return X_3D, y_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Preprocessing\n",
      "Load Labelled Data\n"
     ]
    }
   ],
   "source": [
    "# Save/Load data\n",
    "# if(os.path.exists('./dataset/X_{}.npy'.format(lagWindowSize))):\n",
    "#     X_3D = np.load('./dataset/X_{}.npy'.format(lagWindowSize))\n",
    "#     y_lag = np.load('./dataset/y_{}.npy'.format(lagWindowSize))\n",
    "if(False):\n",
    "    pass\n",
    "else:\n",
    "    start = time.time()\n",
    "    print(\"Load and Preprocessing\")\n",
    "    X_3D, y_lag = load_n_preprocess_data(dataPath,\n",
    "                                         lagWindowSize=lagWindowSize)\n",
    "    y_lag = y_lag.to_numpy()\n",
    "    np.save('./dataset/X_{}.npy'.format(lagWindowSize), X_3D)\n",
    "    np.save('./dataset/y_{}.npy'.format(lagWindowSize), y_lag)\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimension = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16212, 30, 78)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_n_test(X, y, test_size=0.4, val_size=0.1):\n",
    "    # Convert dataframe to numpy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        random_state=2022)\n",
    "    val_size = val_size/(1-test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                      test_size=val_size,\n",
    "                                                      # !!!!!!!!!!!!!!!!!!!!!!!! \n",
    "                                                      shuffle=False,\n",
    "                                                      random_state=2022)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_n_test(X, y, test_size=0.2, val_size=0.2):\n",
    "    # Convert dataframe to numpy\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size+val_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        random_state=2022)\n",
    "    val_size = val_size/(test_size+val_size)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                      test_size=val_size,\n",
    "                                                      # !!!!!!!!!!!!!!!!!!!!!!!! \n",
    "                                                      shuffle=False,\n",
    "                                                      random_state=2022)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_train_n_test(\n",
    "                                                      X_3D, y_lag, \n",
    "                                                      test_size=0.15,\n",
    "                                                      val_size=0.15\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11348, 30, 78)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11348, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2432, 30, 78)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2432, 30, 78)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert datasets into pyTorch format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis import augmentation as aug\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_feature_with_prob(X, prob):\n",
    "    if(random.random()<prob):\n",
    "        X = X.reshape((1, X.shape[0], X.shape[1]))\n",
    "        #X = aug.window_warp(X, 0.05)\n",
    "        X = aug.jitter(X, 0.05)\n",
    "        X = np.squeeze(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchDataset(Dataset):\n",
    "    def __init__(self, X, y, augment_flag=False, num_processes=16):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment_flag = augment_flag\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "    @staticmethod\n",
    "    def augment(X, num_processes):\n",
    "        #X_aug = ts_augmenter.augment(X)\n",
    "        X_aug = augment_feature_with_prob(X, 0.1)\n",
    "        #X_aug = np.stack([ts_augmenter.augment(x) for x in X])\n",
    "#         X_2_aug = [x for x in X]\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             X_aug = np.stack(pool.map(augment_feature, X_2_aug))\n",
    "        return X_aug\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_torch_tensor(X, y):\n",
    "        X = torch.tensor(X).float()\n",
    "        y = torch.tensor(y).float()\n",
    "        return X,y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        Xi, yi = self.X[i], self.y[i]\n",
    "        if(self.augment_flag):\n",
    "            Xi = torchDataset.augment(Xi, self.num_processes)\n",
    "        Xi,yi = torchDataset.to_torch_tensor(Xi,yi)\n",
    "        return Xi,yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = torchDataset(X_train, y_train, True, 1)\n",
    "valSet = torchDataset(X_val, y_val, True)\n",
    "testSet = torchDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(trainSet, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valSet, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testSet, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Features shape: torch.Size([64, 30, 78])\n",
      "Batch Target shape: torch.Size([64, 1])\n",
      "CPU times: user 262 Âµs, sys: 6.87 ms, total: 7.14 ms\n",
      "Wall time: 6.34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, Y = next(iter(train_loader))\n",
    "print(\"Batch Features shape:\", X.shape)\n",
    "print(\"Batch Target shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 78])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on cuda...\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability               \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Run on {}...\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Time-Seise Neural Network Models\n"
     ]
    }
   ],
   "source": [
    "from models import GRU, CNN2DGRU, PatchTSMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = GRU.GRUModel(input_dim=X.shape[-1], \\n                    hidden_dim=10, \\n                    layer_dim=3, \\n                    output_dim=2, \\n                    dropout_prob=0.5)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = GRU.GRUModel(input_dim=X.shape[-1], \n",
    "                    hidden_dim=10, \n",
    "                    layer_dim=3, \n",
    "                    output_dim=2, \n",
    "                    dropout_prob=0.5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2DGRU.CNN2DGRUModel(input_dim=X.shape[-1], \n",
    "                    hidden_dim=20, \n",
    "                    layer_dim=3, \n",
    "                    output_dim=output_dimension, \n",
    "                    dropout_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train, Update, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    #epoch_mses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "        #X, y = X.to(device), y.type(torch.float32).to(device)\n",
    "        #X[:, :, 93:168] = 0\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y.squeeze(dim=1))\n",
    "        #loss = criterion(y_hat.argmax(dim=-1), y.squeeze(dim=1))\n",
    "        accuracy = get_accuracy(y_hat, y.squeeze(dim=1))\n",
    "        #real_mse = real_mean_square_error(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.05)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        #epoch_mses.append(real_mse)\n",
    "        epoch_accs.append(accuracy.item())\n",
    "        scheduler.step()\n",
    "    return epoch_losses, epoch_accs#epoch_mses\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    #epoch_mses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "            #X, y = X.to(device), y.type(torch.float32).to(device)\n",
    "            #X[:, :, 93:168] = 0\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y.squeeze(dim=1))\n",
    "            #loss = criterion(y_hat.argmax(dim=-1), y.squeeze(dim=1))\n",
    "            accuracy = get_accuracy(y_hat, y.squeeze(dim=1))\n",
    "            #real_mse = real_mean_square_error(y_hat, y)\n",
    "            epoch_losses.append(loss.item())\n",
    "            #epoch_mses.append(real_mse)\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return epoch_losses, epoch_accs#epoch_mses\n",
    "\n",
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        num_warmup_steps: int,\n",
    "    ):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self._step_count <= self.num_warmup_steps:\n",
    "            # warmup\n",
    "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
    "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        else:\n",
    "            # every 10 steps, exponentially decay by multipling 0.95\n",
    "            if self._step_count % 2000 == 0:\n",
    "                self.base_lrs = [base_lr * 0.9 for base_lr in self.base_lrs]\n",
    "                print(\"Learning Rate Decay - lr: {}\".format(self.base_lrs[0]))\n",
    "            lr = self.base_lrs\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weight_balance(DataLoader):\n",
    "    label_range = output_dimension\n",
    "    Y = torch.Tensor([])\n",
    "    for _, y in DataLoader:\n",
    "        Y = torch.cat((Y, y.squeeze(dim=1)))\n",
    "    Y = Y.numpy()\n",
    "    totalNum = len(Y)\n",
    "    ratios = np.array([])\n",
    "    for i in range(label_range):\n",
    "        ratio = np.sum((Y == float(i))*1)/totalNum        \n",
    "        ratios = np.append(ratios, ratio)\n",
    "    print(\"Label Distribution: {}\".format(ratios))\n",
    "    ratios = ratios\n",
    "    weights = np.array([])\n",
    "    for i in range(label_range):       \n",
    "        weights = np.append(weights, 1.0/ratios[i])\n",
    "    weights = weights/np.sum(weights)\n",
    "    print(\"Label Weights: {}\".format(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 78])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet.__getitem__(i=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
    "\n",
    "    torch.manual_seed(hparams.SEED)\n",
    "    random.seed(hparams.SEED)\n",
    "    np.random.seed(hparams.SEED)\n",
    "    \n",
    "    train_loader = DataLoader(trainSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(valSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(testSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    X, _ = trainSet.__getitem__(i=0)\n",
    "    feature_size = X.shape[1]\n",
    "    \n",
    "    # Model\n",
    "    if model_type==\"gru\":\n",
    "        model = GRU.GRUModel(\n",
    "            input_dim = feature_size, \n",
    "            hidden_dim = hparams.HIDDEN_DIM, \n",
    "            output_dim = hparams.OUTPUT_DIM,\n",
    "            layer_dim = hparams.N_LAYERS,\n",
    "            dropout_prob = hparams.DROPOUT_RATE, \n",
    "            **kwargs)\n",
    "    elif model_type==\"cnn2dgru\":\n",
    "        model = CNN2DGRU.CNN2DGRUModel(\n",
    "            input_dim = feature_size, \n",
    "            hidden_dim = hparams.HIDDEN_DIM, \n",
    "            output_dim = hparams.OUTPUT_DIM,\n",
    "            layer_dim = hparams.N_LAYERS,\n",
    "            dropout_prob = hparams.DROPOUT_RATE, \n",
    "            **kwargs)\n",
    "    elif model_type=='PatchTSMixer':\n",
    "        model = PatchTSMixer.PatchTSMixer(\n",
    "            input_dim = feature_size, \n",
    "            hidden_dim = hparams.HIDDEN_DIM, \n",
    "            output_dim = hparams.OUTPUT_DIM,\n",
    "            layer_dim = hparams.N_LAYERS,\n",
    "            dropout_prob = hparams.DROPOUT_RATE, \n",
    "            context_length = X.shape[0],\n",
    "            **kwargs)\n",
    "        \n",
    "    num_params = count_parameters(model)\n",
    "    print(f'The model has {num_params:,} trainable parameters')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if hparams.OPTIM == \"sgd\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)        \n",
    "    elif hparams.OPTIM == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
    "    elif hparams.OPTIM == \"adam\":\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
    "    elif hparams.OPTIM == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Optimizer not implemented!\")\n",
    "\n",
    "    \n",
    "    ## ! Balance the loss for imbalanced dataset\n",
    "    weights = loss_weight_balance(train_loader)\n",
    "\n",
    "    #weights[0], weights[1], weights[2], weights[3] = weights[0]*2, weights[1]*2, weights[2]*2, weights[3]*2\n",
    "    weights = torch.Tensor(weights)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Start training\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    # Warmup Scheduler.\n",
    "    WARMUP_STEPS = 100\n",
    "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
    "\n",
    "    train_loss_records = []\n",
    "    valid_loss_records = []\n",
    "    \n",
    "    for epoch in range(hparams.N_EPOCHS):\n",
    "        \n",
    "        # Your code: implement the training process and save the best model.\n",
    "        train_losses, train_acc = train(dataloader=train_loader, \n",
    "                                      model=model, \n",
    "                                      criterion=criterion, \n",
    "                                      optimizer=optimizer, \n",
    "                                      scheduler=lr_scheduler, \n",
    "                                      device=device)\n",
    "        valid_losses, valid_acc = evaluate(dataloader=val_loader,\n",
    "                                         model=model,\n",
    "                                         criterion=criterion,\n",
    "                                         device=device)\n",
    "        \n",
    "        #print(train_real_mses.shape)\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_train_acc = np.mean(train_acc)\n",
    "        epoch_valid_loss = np.mean(valid_losses)\n",
    "        epoch_valid_acc = np.mean(valid_acc)\n",
    "        train_loss_records += train_losses\n",
    "        valid_loss_records += valid_losses\n",
    "        \n",
    "\n",
    "        # Save the model that achieves the smallest validation loss.\n",
    "        if epoch >=5:\n",
    "            if epoch_valid_loss < best_valid_loss:\n",
    "                # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
    "                best_valid_loss = epoch_valid_loss\n",
    "                if not os.path.exists(hparams.CHECKPOINT_FOLDER):\n",
    "                    os.makedirs(hparams.CHECKPOINT_FOLDER)\n",
    "                print(\"Saving ...\")\n",
    "                state = {'state_dict': model.state_dict(),\n",
    "                         'epoch': epoch}\n",
    "                torch.save(state, os.path.join(hparams.CHECKPOINT_FOLDER, model_type+'.pth'))\n",
    "        if epoch%5 == 0:\n",
    "            print(f'epoch: {epoch+1}')\n",
    "            print(f'train_loss: {epoch_train_loss:.5f}, train_acc: {epoch_train_acc:.5f}')\n",
    "            print(f'valid_loss: {epoch_valid_loss:.5f}, valid_acc: {epoch_valid_acc:.5f}')\n",
    "\n",
    "\n",
    "    # Your Code: Load the best model's weights.\n",
    "    state_dict = torch.load('./saved_model/'+model_type+'.pth')['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
    "    test_losses, test_acc = evaluate(dataloader=test_loader, \n",
    "                                   model=model,\n",
    "                                   criterion=criterion,\n",
    "                                   device=device)\n",
    "\n",
    "    epoch_test_loss = np.mean(test_losses)\n",
    "    epoch_test_acc = np.mean(test_acc)\n",
    "    print(f'test_loss: {epoch_test_loss:.5f}, test_acc: {epoch_test_acc:.5f}')\n",
    "    \n",
    "    # Free memory for later usage.\n",
    "    #del model\n",
    "    #torch.cuda.empty_cache()\n",
    "    return {\n",
    "        'model': model,\n",
    "        'num_params': num_params,\n",
    "        \"test_loss\": epoch_test_loss,\n",
    "        \"test_acc\": epoch_test_acc,\n",
    "        \n",
    "        'train_loss_records': train_loss_records,\n",
    "        'valid_loss_records': valid_loss_records\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 128\n",
    "        self.HIDDEN_DIM = 128\n",
    "        self.OUTPUT_DIM = output_dimension\n",
    "        self.N_LAYERS = 3\n",
    "        self.DROPOUT_RATE = 0.1\n",
    "        self.LR = 1e-5\n",
    "        self.N_EPOCHS = 150\n",
    "        self.OPTIM = \"rmsprop\"\n",
    "        self.SEED = 2\n",
    "        self.WD = 0\n",
    "        self.CHECKPOINT_FOLDER = 'saved_model'\n",
    "        \n",
    "hparams = HyperParams()\n",
    "torch.manual_seed(hparams.SEED)\n",
    "random.seed(hparams.SEED)\n",
    "np.random.seed(hparams.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 366,267 trainable parameters\n",
      "Label Distribution: [0.06441664 0.06617906 0.04282693 0.04388438 0.78269299]\n",
      "Label Weights: [0.19889981 0.19360288 0.29916824 0.29195936 0.01636971]\n",
      "epoch: 1\n",
      "train_loss: 1.59539, train_acc: 0.64510\n",
      "valid_loss: 1.55386, valid_acc: 0.78359\n",
      "Saving ...\n",
      "epoch: 6\n",
      "train_loss: 1.05197, train_acc: 0.59942\n",
      "valid_loss: 1.11275, valid_acc: 0.78841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatchTSMixer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#\"cnn2dgru\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgru_l3_h10_rmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m gru_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test_model_with_hparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m gru_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(model)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 88\u001b[0m, in \u001b[0;36mtrain_and_test_model_with_hparams\u001b[0;34m(hparams, model_type, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m valid_loss_records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hparams\u001b[38;5;241m.\u001b[39mN_EPOCHS):\n\u001b[1;32m     86\u001b[0m     \n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Your code: implement the training process and save the best model.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     train_losses, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     valid_losses, valid_acc \u001b[38;5;241m=\u001b[39m evaluate(dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     95\u001b[0m                                      model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m                                      criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m     97\u001b[0m                                      device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m#print(train_real_mses.shape)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, criterion, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#epoch_mses.append(real_mse)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m epoch_accs\u001b[38;5;241m.\u001b[39mappend(accuracy\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_type = \"PatchTSMixer\" #\"cnn2dgru\"\n",
    "title = \"gru_l3_h10_rmsprop\"\n",
    "gru_result = train_and_test_model_with_hparams(hparams, model_type)\n",
    "model = gru_result['model']\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_valid_loss(result, title):\n",
    "    train_loss = result['train_loss_records']\n",
    "    valid_loss = result['valid_loss_records']\n",
    "    valid_iter = [int((len(train_loss)/len(valid_loss))*x) \\\n",
    "                  for x in range(len(valid_loss))]\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(train_loss, 'g', label='Training loss')\n",
    "    plt.plot(valid_iter, valid_loss, 'b', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_valid_loss(gru_result, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    y_heads = torch.tensor([]).to(device)\n",
    "    y_truth = torch.tensor([]) # Let it stays on CPU\n",
    "    model.eval()\n",
    "    #model.train()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_star = model(X)\n",
    "            y_heads = torch.cat((y_heads, y_star), 0)\n",
    "            y_truth = torch.cat((y_truth, y), 0)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_classes = y_heads.argmax(dim=-1)\n",
    "    return predicted_classes.cpu().numpy(), y_truth.cpu().numpy()\n",
    "\n",
    "def get_predict_accuracy(prediction, label):\n",
    "    label = np.reshape(label, prediction.shape)\n",
    "    correct_predictions = np.sum((prediction==label)*1)\n",
    "    accuracy = correct_predictions / len(label)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(trainSet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# y_train_head, y_train  = predict(train_loader, model)\n",
    "\n",
    "start = time.time()\n",
    "#### Testset\n",
    "y_test_head, y_test = predict(test_loader, model)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('Test Set Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_majority(array, window_size):\n",
    "    \"\"\"\n",
    "    Apply moving majority operation to the array.\n",
    "    \n",
    "    Parameters:\n",
    "        array (numpy.ndarray): Input array.\n",
    "        window_size (int): Size of the moving window.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array after applying moving majority operation.\n",
    "    \"\"\"\n",
    "    pad_width = window_size // 2\n",
    "    padded_array = np.pad(array, pad_width, mode='edge')\n",
    "    output = np.zeros_like(array, dtype=int)\n",
    "    unique_labels = np.unique(array)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        window = padded_array[i:i+window_size]\n",
    "        counts = [np.sum(window == label) for label in unique_labels]\n",
    "        majority_label = unique_labels[np.argmax(counts)]\n",
    "        output[i] = majority_label\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_head = moving_majority(y_test_head, window_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_output(y, y_head, title):\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(y, color='g', label='truth')\n",
    "    if y_head is not None:\n",
    "        plt.plot(y_head, color='b', alpha=0.5, label='predict')\n",
    "        accuracy = get_predict_accuracy(y_head, y)\n",
    "        plt.title(title+\"- acc: {:.5f}\".format(accuracy))\n",
    "    #plt.ylim(-0.01, (np.mean(y_head)+np.mean(y))*2)\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_output(y_test, y_test_head, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_test_head, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_test_head, normalize='true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Blossom\", \"Grab\", \"Swipe left\", \"Swipe right\", \"Others\"]\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='.2f')\n",
    "plt.xticks([0,1,2,3,4], classes, fontsize=18, rotation=30)\n",
    "plt.yticks([0,1,2,3,4], classes, fontsize=18, rotation=30)\n",
    "for labels in disp.text_.ravel():\n",
    "    labels.set_fontsize(18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
