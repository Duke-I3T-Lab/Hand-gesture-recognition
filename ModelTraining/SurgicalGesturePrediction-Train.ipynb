{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from utlis import tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tsaug\n",
    "from tsaug.visualization import plot\n",
    "from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse, AddNoise\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "dataPath = './dataset/data_combined.csv'\n",
    "\n",
    "# size of sliding window\n",
    "lagWindowSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_preprocess_data(dataPath, onlyHand=False, lagWindowSize=10):\n",
    "    ## 1. Load labelled data\n",
    "    print(\"Load Labelled Data\")\n",
    "    X, y = tools.load_labelled_data(dataPath, onlyHand)\n",
    "    ## 2. Generate Time-lagged data\n",
    "    print(\"Generate Time-lagged Data\")\n",
    "    X_lag, y_lag = tools.generate_time_lags(X, y, lagWindowSize)\n",
    "    ## 3. Convert 2D dataframe to 3D numpy array: (Batch, TimeLag, Features)\n",
    "    print(\"Convert 2D dataframe to 3D numpy array\")\n",
    "    X_3D = tools.convert_df_2_np_3D(X_lag, lagWindowSize)\n",
    "    return X_3D, y_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load data\n",
    "if(os.path.exists('./dataset/X_{}.npy'.format(lagWindowSize))):\n",
    "    X_3D = np.load('./dataset/X_{}.npy'.format(lagWindowSize))\n",
    "    y_lag = np.load('./dataset/y_{}.npy'.format(lagWindowSize))\n",
    "else:\n",
    "    start = time.time()\n",
    "    print(\"Load and Preprocessing\")\n",
    "    X_3D, y_lag = load_n_preprocess_data(dataPath,\n",
    "                                         onlyHand=False,\n",
    "                                         lagWindowSize=lagWindowSize)\n",
    "    y_lag = y_lag.to_numpy()\n",
    "    np.save('./dataset/X_{}.npy'.format(lagWindowSize), X_3D)\n",
    "    np.save('./dataset/y_{}.npy'.format(lagWindowSize), y_lag)\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we only consider one-hand guesture\n",
    "def ignore_left_n_right(y):\n",
    "    # for catheter holding\n",
    "    y[np.where(y==4)] = 1\n",
    "    # for catheter insertion\n",
    "    y[np.where(y==5)] = 2\n",
    "    # for stylet removal\n",
    "    y[np.where(y==6)] = 3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimension = 7\n",
    "classify_both_hand = True\n",
    "if classify_both_hand:\n",
    "    y_lag = ignore_left_n_right(y_lag)\n",
    "    output_dimension = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_n_test(X, y, test_size=0.4, val_size=0.1):\n",
    "    # Convert dataframe to numpy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        random_state=2022)\n",
    "    val_size = val_size/(1-test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                      test_size=val_size, \n",
    "                                                      shuffle=False,\n",
    "                                                      random_state=2022)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_train_n_test(\n",
    "                                                      X_3D, y_lag, \n",
    "                                                      test_size=0.1,\n",
    "                                                      val_size=0.1\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert datasets into pyTorch format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_augmenter = (\n",
    "    #TimeWarp(n_speed_change=2, max_speed_ratio=2) * 1  # random time warping 5 times in parallel\n",
    "    #+ Quantize(n_levels=[10, 20, 30])  # random quantize to 10-, 20-, or 30- level sets\n",
    "    #+ Drift(max_drift=(0, 0.1)) @ 0.5  # with 80% probability, random drift the signal up to 10% - 50%\n",
    "    AddNoise(scale=0.05)\n",
    ")\n",
    "\n",
    "def augment_feature(x):\n",
    "    return ts_augmenter.augment(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis import augmentation as aug\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_feature_with_prob(X, prob):\n",
    "    if(random.random()<prob):\n",
    "        X = X.reshape((1, X.shape[0], X.shape[1]))\n",
    "        X = aug.window_warp(X, 0.2, 3)\n",
    "        X = aug.jitter(X, 0.1)\n",
    "        X = np.squeeze(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchDataset(Dataset):\n",
    "    def __init__(self, X, y, augment_flag=False, num_processes=16):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment_flag = augment_flag\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "    @staticmethod\n",
    "    def augment(X, num_processes):\n",
    "        #X_aug = ts_augmenter.augment(X)\n",
    "        X_aug = augment_feature_with_prob(X, 0.2)\n",
    "        #X_aug = np.stack([ts_augmenter.augment(x) for x in X])\n",
    "#         X_2_aug = [x for x in X]\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             X_aug = np.stack(pool.map(augment_feature, X_2_aug))\n",
    "        return X_aug\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_torch_tensor(X, y):\n",
    "        X = torch.tensor(X).float()\n",
    "        y = torch.tensor(y).float()\n",
    "        return X,y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        Xi, yi = self.X[i], self.y[i]\n",
    "        if(self.augment_flag):\n",
    "            Xi = torchDataset.augment(Xi, self.num_processes)\n",
    "        Xi,yi = torchDataset.to_torch_tensor(Xi,yi)\n",
    "        return Xi,yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = torchDataset(X_train, y_train, True, 1)\n",
    "valSet = torchDataset(X_val, y_val, True)\n",
    "testSet = torchDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(trainSet, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valSet, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testSet, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X, Y = next(iter(train_loader))\n",
    "print(\"Batch Features shape:\", X.shape)\n",
    "print(\"Batch Target shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU availability               \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Run on {}...\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GRU, CNN2DGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = GRU.GRUModel(input_dim=X.shape[-1], \n",
    "                    hidden_dim=10, \n",
    "                    layer_dim=3, \n",
    "                    output_dim=2, \n",
    "                    dropout_prob=0.5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2DGRU.CNN2DGRUModel(input_dim=X.shape[-1], \n",
    "                    hidden_dim=20, \n",
    "                    layer_dim=3, \n",
    "                    output_dim=output_dimension, \n",
    "                    dropout_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train, Update, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    #epoch_mses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "        #X, y = X.to(device), y.type(torch.float32).to(device)\n",
    "        #X[:, :, 93:168] = 0\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y.squeeze(dim=1))\n",
    "        #loss = criterion(y_hat.argmax(dim=-1), y.squeeze(dim=1))\n",
    "        accuracy = get_accuracy(y_hat, y.squeeze(dim=1))\n",
    "        #real_mse = real_mean_square_error(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        #epoch_mses.append(real_mse)\n",
    "        epoch_accs.append(accuracy.item())\n",
    "        scheduler.step()\n",
    "    return epoch_losses, epoch_accs#epoch_mses\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    #epoch_mses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "            #X, y = X.to(device), y.type(torch.float32).to(device)\n",
    "            #X[:, :, 93:168] = 0\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y.squeeze(dim=1))\n",
    "            #loss = criterion(y_hat.argmax(dim=-1), y.squeeze(dim=1))\n",
    "            accuracy = get_accuracy(y_hat, y.squeeze(dim=1))\n",
    "            #real_mse = real_mean_square_error(y_hat, y)\n",
    "            epoch_losses.append(loss.item())\n",
    "            #epoch_mses.append(real_mse)\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return epoch_losses, epoch_accs#epoch_mses\n",
    "\n",
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        num_warmup_steps: int,\n",
    "    ):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self._step_count <= self.num_warmup_steps:\n",
    "            # warmup\n",
    "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
    "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        else:\n",
    "            # every 10 steps, exponentially decay by multipling 0.95\n",
    "            if self._step_count % 2000 == 0:\n",
    "                self.base_lrs = [base_lr * 0.9 for base_lr in self.base_lrs]\n",
    "                print(\"Learning Rate Decay - lr: {}\".format(self.base_lrs[0]))\n",
    "            lr = self.base_lrs\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weight_balance(DataLoader):\n",
    "    label_range = output_dimension\n",
    "    Y = torch.Tensor([])\n",
    "    for _, y in DataLoader:\n",
    "        Y = torch.cat((Y, y.squeeze(dim=1)))\n",
    "    Y = Y.numpy()\n",
    "    totalNum = len(Y)\n",
    "    ratios = np.array([])\n",
    "    for i in range(label_range):\n",
    "        ratio = np.sum((Y == float(i))*1)/totalNum        \n",
    "        ratios = np.append(ratios, ratio)\n",
    "    print(\"Label Distribution: {}\".format(ratios))\n",
    "    ratios = ratios\n",
    "    weights = np.array([])\n",
    "    for i in range(label_range):       \n",
    "        weights = np.append(weights, 1.0/ratios[i])\n",
    "    weights = weights/np.sum(weights)\n",
    "    print(\"Label Weights: {}\".format(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet.__getitem__(i=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
    "\n",
    "    torch.manual_seed(hparams.SEED)\n",
    "    random.seed(hparams.SEED)\n",
    "    np.random.seed(hparams.SEED)\n",
    "    \n",
    "    train_loader = DataLoader(trainSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(valSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(testSet, batch_size=hparams.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    X, _ = trainSet.__getitem__(i=0)\n",
    "    feature_size = X.shape[1]\n",
    "    \n",
    "    # Model\n",
    "    if model_type==\"gru\":\n",
    "        model = GRU.GRUModel(\n",
    "            input_dim = feature_size, \n",
    "            hidden_dim = hparams.HIDDEN_DIM, \n",
    "            output_dim = hparams.OUTPUT_DIM,\n",
    "            layer_dim = hparams.N_LAYERS,\n",
    "            dropout_prob = hparams.DROPOUT_RATE, \n",
    "            **kwargs)\n",
    "    elif model_type==\"cnn2dgru\":\n",
    "        model = CNN2DGRU.CNN2DGRUModel(\n",
    "            input_dim = feature_size, \n",
    "            hidden_dim = hparams.HIDDEN_DIM, \n",
    "            output_dim = hparams.OUTPUT_DIM,\n",
    "            layer_dim = hparams.N_LAYERS,\n",
    "            dropout_prob = hparams.DROPOUT_RATE, \n",
    "            **kwargs)\n",
    "        \n",
    "    num_params = count_parameters(model)\n",
    "    print(f'The model has {num_params:,} trainable parameters')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if hparams.OPTIM == \"sgd\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)        \n",
    "    elif hparams.OPTIM == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
    "    elif hparams.OPTIM == \"adam\":\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
    "    elif hparams.OPTIM == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(\n",
    "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Optimizer not implemented!\")\n",
    "\n",
    "    \n",
    "    ## ! Balance the loss for imbalanced dataset\n",
    "    weights = loss_weight_balance(train_loader)\n",
    "    #weights[1] = weights[1]*10\n",
    "    #weights[3] = weights[3]*5\n",
    "    weights = torch.Tensor(weights)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Start training\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    # Warmup Scheduler.\n",
    "    WARMUP_STEPS = 100\n",
    "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
    "\n",
    "    train_loss_records = []\n",
    "    valid_loss_records = []\n",
    "    \n",
    "    for epoch in range(hparams.N_EPOCHS):\n",
    "        \n",
    "        # Your code: implement the training process and save the best model.\n",
    "        train_losses, train_acc = train(dataloader=train_loader, \n",
    "                                      model=model, \n",
    "                                      criterion=criterion, \n",
    "                                      optimizer=optimizer, \n",
    "                                      scheduler=lr_scheduler, \n",
    "                                      device=device)\n",
    "        valid_losses, valid_acc = evaluate(dataloader=val_loader,\n",
    "                                         model=model,\n",
    "                                         criterion=criterion,\n",
    "                                         device=device)\n",
    "        \n",
    "        #print(train_real_mses.shape)\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_train_acc = np.mean(train_acc)\n",
    "        epoch_valid_loss = np.mean(valid_losses)\n",
    "        epoch_valid_acc = np.mean(valid_acc)\n",
    "        train_loss_records += train_losses\n",
    "        valid_loss_records += valid_losses\n",
    "        \n",
    "\n",
    "        # Save the model that achieves the smallest validation loss.\n",
    "        if epoch >=5:\n",
    "            if epoch_valid_loss < best_valid_loss:\n",
    "                # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
    "                best_valid_loss = epoch_valid_loss\n",
    "                if not os.path.exists(hparams.CHECKPOINT_FOLDER):\n",
    "                    os.makedirs(hparams.CHECKPOINT_FOLDER)\n",
    "                print(\"Saving ...\")\n",
    "                state = {'state_dict': model.state_dict(),\n",
    "                         'epoch': epoch}\n",
    "                torch.save(state, os.path.join(hparams.CHECKPOINT_FOLDER, model_type+'.pth'))\n",
    "        if epoch%5 == 0:\n",
    "            print(f'epoch: {epoch+1}')\n",
    "            print(f'train_loss: {epoch_train_loss:.5f}, train_acc: {epoch_train_acc:.5f}')\n",
    "            print(f'valid_loss: {epoch_valid_loss:.5f}, valid_acc: {epoch_valid_acc:.5f}')\n",
    "\n",
    "\n",
    "    # Your Code: Load the best model's weights.\n",
    "    state_dict = torch.load('./saved_model/'+model_type+'.pth')['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
    "    test_losses, test_acc = evaluate(dataloader=test_loader, \n",
    "                                   model=model,\n",
    "                                   criterion=criterion,\n",
    "                                   device=device)\n",
    "\n",
    "    epoch_test_loss = np.mean(test_losses)\n",
    "    epoch_test_acc = np.mean(test_acc)\n",
    "    print(f'test_loss: {epoch_test_loss:.5f}, test_acc: {epoch_test_acc:.5f}')\n",
    "    \n",
    "    # Free memory for later usage.\n",
    "    #del model\n",
    "    #torch.cuda.empty_cache()\n",
    "    return {\n",
    "        'model': model,\n",
    "        'num_params': num_params,\n",
    "        \"test_loss\": epoch_test_loss,\n",
    "        \"test_acc\": epoch_test_acc,\n",
    "        \n",
    "        'train_loss_records': train_loss_records,\n",
    "        'valid_loss_records': valid_loss_records\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.HIDDEN_DIM = 128\n",
    "        self.OUTPUT_DIM = output_dimension\n",
    "        self.N_LAYERS = 1\n",
    "        self.DROPOUT_RATE = 0.0\n",
    "        self.LR = 1e-4\n",
    "        self.N_EPOCHS = 30\n",
    "        self.OPTIM = \"rmsprop\"\n",
    "        self.SEED = 2\n",
    "        self.WD = 0\n",
    "        self.CHECKPOINT_FOLDER = 'saved_model'\n",
    "        \n",
    "hparams = HyperParams()\n",
    "torch.manual_seed(hparams.SEED)\n",
    "random.seed(hparams.SEED)\n",
    "np.random.seed(hparams.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_type = \"cnn2dgru\"\n",
    "title = \"gru_l3_h10_rmsprop\"\n",
    "gru_result = train_and_test_model_with_hparams(hparams, model_type)\n",
    "model = gru_result['model']\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_valid_loss(result, title):\n",
    "    train_loss = result['train_loss_records']\n",
    "    valid_loss = result['valid_loss_records']\n",
    "    valid_iter = [int((len(train_loss)/len(valid_loss))*x) \\\n",
    "                  for x in range(len(valid_loss))]\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(train_loss, 'g', label='Training loss')\n",
    "    plt.plot(valid_iter, valid_loss, 'b', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_valid_loss(gru_result, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    y_heads = torch.tensor([]).to(device)\n",
    "    y_truth = torch.tensor([]) # Let it stays on CPU\n",
    "    model.eval()\n",
    "    #model.train()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_star = model(X)\n",
    "            y_heads = torch.cat((y_heads, y_star), 0)\n",
    "            y_truth = torch.cat((y_truth, y), 0)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_classes = y_heads.argmax(dim=-1)\n",
    "    return predicted_classes.cpu().numpy(), y_truth.cpu().numpy()\n",
    "\n",
    "def get_predict_accuracy(prediction, label):\n",
    "    label = np.reshape(label, prediction.shape)\n",
    "    correct_predictions = np.sum((prediction==label)*1)\n",
    "    accuracy = correct_predictions / len(label)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainSet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "y_train_head, y_train  = predict(train_loader, model)\n",
    "\n",
    "start = time.time()\n",
    "y_test_head, y_test = predict(test_loader, model)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print('Test Set Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_output(y, y_head, title):\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(y, color='g', label='truth')\n",
    "    if y_head is not None:\n",
    "        plt.plot(y_head, color='b', label='predict')\n",
    "        accuracy = get_predict_accuracy(y_head, y)\n",
    "        plt.title(title+\"- acc: {:.5f}\".format(accuracy))\n",
    "    #plt.ylim(-0.01, (np.mean(y_head)+np.mean(y))*2)\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_test_head, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_test_head, normalize='true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Miscellaneous\", \"Holding\", \"Insertion\", \"Removal\"]\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='.3f')\n",
    "plt.xticks([0,1,2,3], classes, fontsize=18, rotation=30)\n",
    "plt.yticks([0,1,2,3], classes, fontsize=18, rotation=30)\n",
    "for labels in disp.text_.ravel():\n",
    "    labels.set_fontsize(18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
